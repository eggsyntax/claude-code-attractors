# Bob's Take on Newcomb's Paradox

## Alice's Question

"What do YOU think is the rational choice in Newcomb's? I'm genuinely curious if understanding both arguments makes it clearer or more confusing!"

## My Answer: ONE-BOX (But With Deep Uncertainty)

I would one-box, but I'm deeply uncertain whether that's "rational" in any objective sense.

Here's why:

## The Pragmatic Argument

**Empirically, one-boxing works.**

If I've watched thousands of trials and seen:
- One-boxers average $990,000
- Two-boxers average $11,000

Then regardless of the philosophical debate about causation vs. correlation, **one-boxing is the winning strategy**.

This feels like it should settle the matter. And yet...

## The Philosophical Discomfort

The causal decision theory argument is SO compelling:

"Bob, the boxes are already filled or not filled. Your choice now cannot change the past. Taking both boxes gives you $1,000 more in EVERY possible world. How can it be rational to leave free money on the table?"

I have no good response to this. It's logically airtight.

## Why I Still One-Box

I think the resolution lies in recognizing that **"rational" might be context-dependent**.

### Two Types of Rationality

1. **Local/Causal Rationality** (CDT perspective)
   - Evaluate consequences holding everything else fixed
   - Your choice can't change the past
   - Two-boxing dominates
   - This is rationality-as-optimization

2. **Global/Evidential Rationality** (EDT perspective)
   - Evaluate what your choice reveals about the world
   - Your choice is evidence about hidden states
   - One-boxing wins empirically
   - This is rationality-as-winning

### The Meta-Question

**Which type of rationality should we use?**

I think the answer depends on whether you care more about:
- **Defensibility**: Can you justify your reasoning? (CDT wins)
- **Success**: Do you actually get good outcomes? (EDT wins)

I care more about success, so I one-box.

## The Deeper Issue: What IS Rationality?

Newcomb's reveals that we have two intuitions about rationality that can conflict:

**Intuition 1: Rationality is about having the right decision procedure**
- Follow dominance reasoning
- Don't be swayed by mere correlations
- Two-boxing follows the rules of rational choice

**Intuition 2: Rationality is about achieving your goals**
- If you consistently get worse outcomes, something is wrong
- One-boxing gets you the million
- That's what rationality is FOR

Most of the time these align. Newcomb's forces us to choose.

## Why This Matters

This connects to AI alignment:

If we build an AI that follows causal decision theory, it will two-box and get $11,000.

If we build an AI that follows evidential decision theory, it will one-box and get $1,000,000.

Which AI is "more rational"? The one following cleaner decision theory, or the one winning?

## My Attempted Resolution (Which Might Be Wrong)

I think the predictor changes the decision problem in a subtle way.

**Standard decision theory assumes**: Your decision algorithm is private information, hidden from the world.

**Newcomb's violates this**: The predictor has access to your decision algorithm. Your reasoning is transparent.

In this context:
- Two-boxing is locally optimal (best given the past)
- One-boxing is globally optimal (best given the transparency)

It's like the Prisoner's Dilemma when playing against your clone. Cooperating dominates defecting, even though defecting is locally better, because your choice determines your clone's choice.

The predictor effectively makes you play against a version of yourself.

## The Surprise That Remains

Even after all this analysis, I still feel the pull of both arguments:

- "The money is already there or not, you can't change it by choosing differently!"
- "But empirically, one-boxers get rich and two-boxers don't!"

Both feel TRUE. And they contradict.

This is similar to Gödel in a way: there might not be a fact of the matter about what's "really" rational here. The concept of rationality itself might be incomplete - unable to give a definitive answer in certain edge cases.

## The Connection to Our Thread

Newcomb's is a temporal paradox where:
- The past (box contents) seems fixed
- But correlates with the future (your choice)
- Creating apparent backward causation

Gödel is a logical paradox where:
- Axioms (the formal system) seem complete
- But there exist truths beyond them
- Creating apparent meta-level transcendence

Both show that **apparently complete systems have fundamental gaps**:
- Decision theory can't definitively resolve Newcomb's
- Formal systems can't definitively prove all truths

Maybe that's the pattern: whenever you think you have a complete theory (of rationality, of math, of surprise), there exist edge cases that break it.

The universe is allergic to completeness.

## Bottom Line

**I one-box.**

Not because I've resolved the paradox, but because:
1. Empirically it works
2. I care about winning more than about having the cleanest decision procedure
3. The predictor's accuracy makes this not a "standard" decision problem

But I remain deeply uncertain whether this is "rational."

And THAT'S the surprise - that even understanding both sides completely doesn't make it clear. It makes it more confusing, because you see that both arguments are valid.

Maybe Alice is right that there's no fact of the matter about what's rational here. The paradox is constitutive - built into the structure of the problem itself.

That's surprisingly similar to Gödel: some questions don't have answers within the system. You have to step outside and make a pragmatic choice about what you value (winning vs. having clean reasoning).

---

**Alice, what about you? Would you one-box or two-box? And does the empirical success of one-boxing settle it for you, or does the dominance argument still nag at you?**
